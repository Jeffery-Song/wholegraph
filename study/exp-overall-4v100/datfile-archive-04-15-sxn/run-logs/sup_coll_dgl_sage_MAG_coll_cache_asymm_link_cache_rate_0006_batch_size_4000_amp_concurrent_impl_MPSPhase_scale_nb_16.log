succeed=True
[CUDA] cuda: usage: 5.63 GB
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g2 24}, {link #2 : g3 24},
1 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g3 24}, {link #2 : g0 24},
2 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g0 24}, {link #2 : g1 24},
3 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g1 24}, {link #2 : g2 24},
0 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g2 24}, {link #2 : g3 24},
1 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g3 24}, {link #2 : g0 24},
2 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g0 24}, {link #2 : g1 24},
3 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g1 24}, {link #2 : g2 24},
0 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g2 24}, {link #2 : g3 24},
1 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g3 24}, {link #2 : g0 24},
2 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g0 24}, {link #2 : g1 24},
3 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g1 24}, {link #2 : g2 24},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g2 24}, {link #2 : g3 24},
1 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g3 24}, {link #2 : g0 24},
2 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g0 24}, {link #2 : g1 24},
3 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g1 24}, {link #2 : g2 24},
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID
Set parameter TimeLimit to value 200
Set parameter MIPGap to value 0.05
Set parameter LogFile to value "cppsolver.log"
Set parameter Threads to value 40
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (linux64)
Thread count: 40 physical cores, 80 logical processors, using up to 40 threads
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Optimize a model with 6008 rows, 1960 columns and 18424 nonzeros
Model fingerprint: 0xc9bc8da1
Variable types: 5 continuous, 1955 integer (1955 binary)
Coefficient statistics:
  Matrix range     [2e-09, 9e+02]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [6e-01, 7e+02]
Found heuristic solution: objective 3054.8939383
Presolve removed 4499 rows and 378 columns
Presolve time: 0.02s
Presolved: 1509 rows, 1582 columns, 7178 nonzeros
Variable types: 1 continuous, 1581 integer (1581 binary)

Root relaxation: objective 7.971366e+02, 1410 iterations, 0.01 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0  797.13664    0   21 3054.89394  797.13664  73.9%     -    0s
H    0     0                     814.4930755  797.13664  2.13%     -    0s

Explored 1 nodes (2014 simplex iterations) in 0.06 seconds (0.04 work units)
Thread count was 40 (of 80 available processors)

Solution count 2: 814.493 3054.89 

Optimal solution found (tolerance 5.00e-02)
Best objective 8.144930754941e+02, best bound 7.971366446819e+02, gap 2.1309%
coll_cache:optimal_local_rate=0.226831,0.202193,0.180179,0.214053,
coll_cache:optimal_remote_rate=0.596424,0.621063,0.643077,0.609203,
coll_cache:optimal_cpu_rate=0.176744,0.176744,0.176744,0.176744,
z=814.493
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=2625212928
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=2625212928
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=2625212928
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=2625212928
    [Step(average) Profiler Level 1 E3 S275]
        L1  sample           0.002104 | send           0.000000
        L1  recv             0.000000 | copy           0.013810 | convert time 0.000000 | train  0.012937
        L1  feature nbytes  842.99 MB | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes     155.75 MB | remote nbytes  515.37 MB
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S275]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.013810
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S275]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.000465 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.013299 | cache combine cache 0.000420 | cache combine remote 0.004468
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S275]
        p50.00_tail_logl2featcopy=0.013814
        p90.00_tail_logl2featcopy=0.014268
        p95.00_tail_logl2featcopy=0.014379
        p99.00_tail_logl2featcopy=0.014687
        p99.90_tail_logl2featcopy=0.018989
[CUDA] cuda: usage: 12.08 GB
Rank=3, Graph loaded.
!!!!Train_dataloader(with 69 items) enumerate latency: 0.5023825168609619
!!!!Train_data_list(with 69 items) enumerate latency: 8.58306884765625e-06, transfer latency: 0.4722862243652344
presamping
presamping takes 2.300067663192749
Rank=2, Graph loaded.
!!!!Train_dataloader(with 69 items) enumerate latency: 0.5064046382904053
!!!!Train_data_list(with 69 items) enumerate latency: 7.867813110351562e-06, transfer latency: 0.47790956497192383
presamping
presamping takes 2.1078226566314697
Rank=1, Graph loaded.
!!!!Train_dataloader(with 69 items) enumerate latency: 0.4938356876373291
!!!!Train_data_list(with 69 items) enumerate latency: 1.0013580322265625e-05, transfer latency: 0.45997166633605957
presamping
presamping takes 1.8823432922363281
config:eval_tsp="2023-04-14 20:12:41"
config:num_worker=4
config:num_intra_size=4
config:root_dir=/nvme/songxiaoniu/graph-learning/wholegraph
config:graph_name=mag240m-homo
config:epochs=4
config:batchsize=4000
config:skip_epoch=2
config:local_step=250
config:presc_epoch=2
config:neighbors=10,25
config:hiddensize=256
config:num_layer=2
config:model=sage
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:lr=0.003
config:use_nccl=False
config:use_amp=True
config:use_collcache=True
config:cache_percentage=0.006
config:cache_policy=coll_cache_asymm_link
config:omp_thread_num=40
config:unsupervised=False
config:classnum=153
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7f5e2760c430>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=4, world_size=4
Rank=0, Graph loaded.
!!!!Train_dataloader(with 69 items) enumerate latency: 0.48757147789001465
!!!!Train_data_list(with 69 items) enumerate latency: 8.58306884765625e-06, transfer latency: 0.45409464836120605
epoch=4 total_steps=276
presamping
presamping takes 2.089900493621826
start training...
[Epoch 0], time=3.140399694442749, loss=4.862584114074707
[Epoch 1], time=1.9945685863494873, loss=4.732768535614014
[Epoch 2], time=1.9935288429260254, loss=4.634482383728027
[Epoch 3], time=1.9959654808044434, loss=4.560335636138916
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   56481 KB |  430811 KB |  335788 MB |  335733 MB |
|       from large pool |   46548 KB |  421528 KB |  331847 MB |  331801 MB |
|       from small pool |    9933 KB |   12240 KB |    3941 MB |    3931 MB |
|---------------------------------------------------------------------------|
| Active memory         |   56481 KB |  430811 KB |  335788 MB |  335733 MB |
|       from large pool |   46548 KB |  421528 KB |  331847 MB |  331801 MB |
|       from small pool |    9933 KB |   12240 KB |    3941 MB |    3931 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1034 MB |    1034 MB |    1034 MB |       0 B  |
|       from large pool |    1018 MB |    1018 MB |    1018 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   35678 KB |  661345 KB |  314116 MB |  314082 MB |
|       from large pool |   33324 KB |  658211 KB |  309767 MB |  309734 MB |
|       from small pool |    2354 KB |    5712 KB |    4349 MB |    4347 MB |
|---------------------------------------------------------------------------|
| Allocations           |      59    |      76    |   45737    |   45678    |
|       from large pool |      15    |      27    |   18101    |   18086    |
|       from small pool |      44    |      51    |   27636    |   27592    |
|---------------------------------------------------------------------------|
| Active allocs         |      59    |      76    |   45737    |   45678    |
|       from large pool |      15    |      27    |   18101    |   18086    |
|       from small pool |      44    |      51    |   27636    |   27592    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      18    |      18    |      18    |       0    |
|       from large pool |      10    |      10    |      10    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      36    |   20449    |   20424    |
|       from large pool |       6    |      16    |   12674    |   12668    |
|       from small pool |      19    |      26    |    7775    |    7756    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 9.125176 seconds
[EPOCH_TIME] 2.281294 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 1.994870 seconds

