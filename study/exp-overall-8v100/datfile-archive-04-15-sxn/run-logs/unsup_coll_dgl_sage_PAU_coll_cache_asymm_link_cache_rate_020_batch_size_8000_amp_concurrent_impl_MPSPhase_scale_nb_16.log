succeed=True
[CUDA] cuda: usage: 5.69 GB
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID
Set parameter TimeLimit to value 200
Set parameter MIPGap to value 0.05
Set parameter ScaleFlag to value 1
Set parameter DegenMoves to value 2
Set parameter Cuts to value 2
Set parameter LogFile to value "cppsolver.log"
Set parameter Threads to value 48
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (linux64)
Thread count: 48 physical cores, 96 logical processors, using up to 48 threads
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Optimize a model with 116224 rows, 18049 columns and 292216 nonzeros
Model fingerprint: 0x589cc780
Variable types: 9 continuous, 18040 integer (18040 binary)
Coefficient statistics:
  Matrix range     [5e-09, 9e+04]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+05]
Warning: Model contains large matrix coefficient range
         Consider reformulating model or setting NumericFocus parameter
         to avoid numerical issues.
Found heuristic solution: objective 700667.55578
Presolve removed 98574 rows and 7 columns
Presolve time: 0.30s
Presolved: 17650 rows, 18042 columns, 86235 nonzeros
Variable types: 1 continuous, 18041 integer (18040 binary)

Root relaxation: objective 2.423457e+04, 26954 iterations, 4.25 seconds (5.04 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0 24234.5715    0 7553 700667.556 24234.5715  96.5%     -    4s
H    0     0                    475214.02168 24234.5715  94.9%     -    4s
H    0     0                    471746.83589 24234.5715  94.9%     -    4s
H    0     0                    255688.97433 24234.5715  90.5%     -    8s
H    0     0                    216057.08026 24234.5715  88.8%     -    8s
H    0     0                    190243.85390 25033.1461  86.8%     -   12s
H    0     0                    187090.40061 25033.1461  86.6%     -   12s
H    0     0                    183503.06844 25033.1461  86.4%     -   12s
H    0     0                    106339.06378 25033.1461  76.5%     -   13s
     0     0 25192.0163    0 5713 106339.064 25192.0163  76.3%     -   13s
H    0     0                    79818.581682 25192.0163  68.4%     -   14s
H    0     0                    79330.651697 25521.9860  67.8%     -   18s
     0     0 25521.9860    0 5500 79330.6517 25521.9860  67.8%     -   18s
     0     0 25688.1411    0 5858 79330.6517 25688.1411  67.6%     -   21s
H    0     0                    70781.019746 25872.3564  63.4%     -   24s
H    0     0                    60917.552094 25872.3564  57.5%     -   24s
     0     0 25872.3564    0 6291 60917.5521 25872.3564  57.5%     -   24s
     0     0 25912.3599    0 6516 60917.5521 25912.3599  57.5%     -   26s
     0     0 25926.2772    0 6640 60917.5521 25926.2772  57.4%     -   28s
     0     0 25929.4969    0 6602 60917.5521 25929.4969  57.4%     -   29s
H    0     0                    60039.442007 25930.6960  56.8%     -   30s
     0     0 25930.6960    0 6604 60039.4420 25930.6960  56.8%     -   30s
     0     0 25931.2594    0 6691 60039.4420 25931.2594  56.8%     -   32s
H    0     0                    54463.056854 25931.8207  52.4%     -   33s
     0     0 25931.8207    0 6657 54463.0569 25931.8207  52.4%     -   33s
     0     0 25932.0793    0 6653 54463.0569 25932.0793  52.4%     -   33s
     0     0 25932.0797    0 6652 54463.0569 25932.0797  52.4%     -   33s
H    0     0                    52818.922873 25932.0797  50.9%     -   37s
H    0     0                    51170.956407 26550.9641  48.1%     -   56s
H    0     0                    48883.206252 26550.9641  45.7%     -   56s
H    0     0                    48444.239786 26550.9641  45.2%     -   56s
H    0     0                    47518.349625 26550.9641  44.1%     -   56s
H    0     0                    46446.469705 26550.9641  42.8%     -   56s
H    0     0                    46006.950748 26550.9641  42.3%     -   56s
H    0     0                    35086.426062 26550.9641  24.3%     -   56s
H    0     0                    34958.250896 26550.9641  24.0%     -   56s
H    0     0                    34210.446356 26550.9641  22.4%     -   56s
H    0     0                    33704.865109 26550.9641  21.2%     -   56s
H    0     0                    30911.737316 26550.9641  14.1%     -   56s
H    0     0                    30766.573544 26550.9641  13.7%     -   56s
H    0     0                    29743.574174 26550.9641  10.7%     -   56s
     0     0 26550.9641    0 2058 29743.5742 26550.9641  10.7%     -   56s
H    0     0                    29605.041758 26550.9641  10.3%     -   56s
H    0     0                    28820.548682 26610.0317  7.67%     -   61s
H    0     0                    28666.546212 26610.0317  7.17%     -   61s
     0     0 26610.0317    0 1629 28666.5462 26610.0317  7.17%     -   61s
     0     0 26610.4492    0 1558 28666.5462 26610.4492  7.17%     -   63s
H    0     0                    27749.526235 26610.4492  4.10%     -   68s

Cutting planes:
  Gomory: 53
  Lift-and-project: 254
  MIR: 3
  Flow cover: 2
  Zero half: 4355

Explored 1 nodes (119643 simplex iterations) in 68.20 seconds (71.20 work units)
Thread count was 48 (of 96 available processors)

Solution count 10: 27749.5 28666.5 28820.5 ... 34958.3

Optimal solution found (tolerance 5.00e-02)
Best objective 2.774952623499e+04, best bound 2.661044922405e+04, gap 4.1049%
coll_cache:optimal_local_rate=0.358283,0.363161,0.358678,0.359176,0.36059,0.354317,0.356485,0.365372,
coll_cache:optimal_remote_rate=0.626502,0.621623,0.626106,0.625608,0.624195,0.630467,0.6283,0.619413,
coll_cache:optimal_cpu_rate=0.0152158,0.0152158,0.0152158,0.0152158,0.0152158,0.0152158,0.0152158,0.0152158,
z=27749.5
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=11429402112
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=11429402112
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=11429402112
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=11429402112
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=11429402112
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=11429402112
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=11429402112
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=11429402112
    [Step(average) Profiler Level 1 E3 S999]
        L1  sample           0.014721 | send           0.000000
        L1  recv             0.000000 | copy           0.022683 | convert time 0.000000 | train  0.023693
        L1  feature nbytes    2.03 GB | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes      35.28 MB | remote nbytes    1.27 GB
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S999]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.022683
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S999]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.003570 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.009999 | cache combine cache 0.001465 | cache combine remote 0.017572
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S999]
        p50.00_tail_logl2featcopy=0.022812
        p90.00_tail_logl2featcopy=0.024071
        p95.00_tail_logl2featcopy=0.024236
        p99.00_tail_logl2featcopy=0.024581
        p99.90_tail_logl2featcopy=0.048503
[CUDA] cuda: usage: 24.20 GB
Rank=1, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.021288, per step: 0.000170
presamping
presamping takes 38.776365995407104
Rank=2, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.010392, per step: 0.000083
presamping
presamping takes 38.66006517410278
creating_intra_node_communicator root=4, local_size=4, world_size=8
Rank=4, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.011155, per step: 0.000089
presamping
presamping takes 37.92933797836304
Rank=6, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.007811, per step: 0.000062
presamping
presamping takes 37.946110010147095
config:eval_tsp="2023-04-15 00:18:51"
config:num_worker=8
config:num_intra_size=4
config:root_dir=/nvme/songxiaoniu/graph-learning/wholegraph
config:graph_name=ogbn-papers100M
config:epochs=4
config:batchsize=8000
config:skip_epoch=2
config:local_step=125
config:presc_epoch=2
config:neighbors=10,25
config:hiddensize=256
config:num_layer=2
config:model=sage
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:lr=0.003
config:use_nccl=False
config:use_amp=True
config:use_collcache=True
config:cache_percentage=0.2
config:cache_policy=coll_cache_asymm_link
config:omp_thread_num=48
config:unsupervised=True
config:classnum=172
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7fdedd4e4400>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=4, world_size=8
Rank=0, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.007518, per step: 0.000060
epoch=4 total_steps=500
presamping
presamping takes 36.10651612281799
start training...
[Epoch 0], time=9.224219560623169, loss=0.6931472420692444
[Epoch 1], time=7.63425874710083, loss=0.6931472420692444
[Epoch 2], time=7.675170660018921, loss=0.6931472420692444
[Epoch 3], time=7.6293885707855225, loss=0.6931472420692444
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  272315 KB |    1501 MB |    2337 GB |    2337 GB |
|       from large pool |  259177 KB |    1490 MB |    2323 GB |    2322 GB |
|       from small pool |   13138 KB |      18 MB |      14 GB |      14 GB |
|---------------------------------------------------------------------------|
| Active memory         |  272315 KB |    1501 MB |    2337 GB |    2337 GB |
|       from large pool |  259177 KB |    1490 MB |    2323 GB |    2322 GB |
|       from small pool |   13138 KB |      18 MB |      14 GB |      14 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1866 MB |    1866 MB |    1866 MB |       0 B  |
|       from large pool |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from small pool |      24 MB |      24 MB |      24 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  102468 KB |    1140 MB |    2023 GB |    2023 GB |
|       from large pool |   99223 KB |    1132 MB |    2007 GB |    2007 GB |
|       from small pool |    3245 KB |      12 MB |      15 GB |      15 GB |
|---------------------------------------------------------------------------|
| Allocations           |      58    |      74    |  102430    |  102372    |
|       from large pool |      13    |      24    |   38764    |   38751    |
|       from small pool |      45    |      54    |   63666    |   63621    |
|---------------------------------------------------------------------------|
| Active allocs         |      58    |      74    |  102430    |  102372    |
|       from large pool |      13    |      24    |   38764    |   38751    |
|       from small pool |      45    |      54    |   63666    |   63621    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      32    |      32    |      32    |       0    |
|       from large pool |      20    |      20    |      20    |       0    |
|       from small pool |      12    |      12    |      12    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      43    |   38431    |   38408    |
|       from large pool |       7    |      20    |   22003    |   21996    |
|       from small pool |      16    |      27    |   16428    |   16412    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 32.164698 seconds
[EPOCH_TIME] 8.041175 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 7.652542 seconds
Rank=5, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.007640, per step: 0.000061
presamping
presamping takes 37.916099071502686
Rank=7, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.007338, per step: 0.000059
presamping
presamping takes 35.87171149253845
Rank=3, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.021259, per step: 0.000170
presamping
presamping takes 38.74289870262146

