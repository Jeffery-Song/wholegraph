succeed=True
[CUDA] cuda: usage: 5.45 GB
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID
Set parameter TimeLimit to value 200
Set parameter MIPGap to value 0.05
Set parameter ScaleFlag to value 1
Set parameter DegenMoves to value 2
Set parameter Cuts to value 2
Set parameter LogFile to value "cppsolver.log"
Set parameter Threads to value 48
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (linux64)
Thread count: 48 physical cores, 96 logical processors, using up to 48 threads
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Optimize a model with 125464 rows, 19484 columns and 315456 nonzeros
Model fingerprint: 0xbea9ba0d
Variable types: 9 continuous, 19475 integer (19475 binary)
Coefficient statistics:
  Matrix range     [2e-09, 3e+03]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 2e+03]
Warning: Model contains large matrix coefficient range
         Consider reformulating model or setting NumericFocus parameter
         to avoid numerical issues.
Found heuristic solution: objective 11025.785678
Presolve removed 106774 rows and 376 columns
Presolve time: 0.33s
Presolved: 18690 rows, 19108 columns, 89925 nonzeros
Variable types: 1 continuous, 19107 integer (19106 binary)

Root simplex log...

Iteration    Objective       Primal Inf.    Dual Inf.      Time
   27499    3.7925084e+02   1.188691e+07   0.000000e+00      5s
   28782    3.8304012e+02   0.000000e+00   0.000000e+00      6s

Root relaxation: objective 3.830401e+02, 28782 iterations, 5.42 seconds (6.18 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0  383.04012    0 9137 11025.7857  383.04012  96.5%     -    6s
H    0     0                    7583.9186863  383.04012  94.9%     -    6s
H    0     0                    7497.9637780  383.04012  94.9%     -    6s
H    0     0                    5398.2364291  383.04012  92.9%     -   10s
H    0     0                    2855.7004337  383.04012  86.6%     -   10s
H    0     0                    1978.8163061  383.04012  80.6%     -   10s
H    0     0                    1829.0436511  403.28697  78.0%     -   16s
     0     0  403.28697    0 8005 1829.04365  403.28697  78.0%     -   16s
H    0     0                    1200.1793720  403.28697  66.4%     -   18s
     0     0  417.41524    0 8053 1200.17937  417.41524  65.2%     -   20s
     0     0  417.41524    0 8362 1200.17937  417.41524  65.2%     -   23s
     0     0  422.09812    0 8274 1200.17937  422.09812  64.8%     -   24s
     0     0  426.66117    0 8589 1200.17937  426.66117  64.5%     -   28s
     0     0  428.42191    0 8797 1200.17937  428.42191  64.3%     -   31s
     0     0  428.73343    0 8863 1200.17937  428.73343  64.3%     -   32s
     0     0  428.87237    0 8870 1200.17937  428.87237  64.3%     -   33s
     0     0  428.94977    0 8889 1200.17937  428.94977  64.3%     -   34s
     0     0  429.01793    0 8856 1200.17937  429.01793  64.3%     -   34s
     0     0  429.04581    0 8906 1200.17937  429.04581  64.3%     -   35s
     0     0  429.06615    0 8934 1200.17937  429.06615  64.2%     -   35s
     0     0  429.07127    0 8925 1200.17937  429.07127  64.2%     -   36s
     0     0  442.98745    0 7953 1200.17937  442.98745  63.1%     -   53s
H    0     0                    1095.9216250  467.42627  57.3%     -   88s
H    0     0                    1009.3530199  467.42627  53.7%     -   88s
H    0     0                     923.0850247  467.42627  49.4%     -   88s
H    0     0                     839.4930477  467.42627  44.3%     -   88s
     0     0  467.42627    0 1481  839.49305  467.42627  44.3%     -   93s
     0     0  467.43000    0 1508  839.49305  467.43000  44.3%     -   94s
H    0     0                     820.2454986  467.43000  43.0%     -   98s
     0     0  470.88839    0 1045  820.24550  470.88839  42.6%     -  104s
H    0     0                     672.6453719  470.88839  30.0%     -  104s
H    0     0                     481.0413551  470.88839  2.11%     -  110s

Cutting planes:
  Gomory: 98
  Lift-and-project: 201
  MIR: 1
  Flow cover: 5
  Zero half: 6150

Explored 1 nodes (150297 simplex iterations) in 110.27 seconds (115.20 work units)
Thread count was 48 (of 96 available processors)

Solution count 10: 481.041 672.645 820.245 ... 1978.82

Optimal solution found (tolerance 5.00e-02)
Best objective 4.810413551152e+02, best bound 4.708883949509e+02, gap 2.1106%
coll_cache:optimal_local_rate=0.257919,0.273011,0.264116,0.265448,0.26504,0.265876,0.259257,0.244258,
coll_cache:optimal_remote_rate=0.722239,0.707147,0.716043,0.714711,0.715118,0.714282,0.720902,0.735901,
coll_cache:optimal_cpu_rate=0.0198415,0.0198415,0.0198415,0.0198415,0.0198415,0.0198415,0.0198415,0.0198415,
z=481.041
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=7875640320
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=7875640320
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=7875640320
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=7875640320
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=7875640320
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=7875640320
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=7875640320
test_result:init:feat_nbytes=375030526464
test_result:init:cache_nbytes=7875640320
    [Step(average) Profiler Level 1 E3 S135]
        L1  sample           0.004079 | send           0.000000
        L1  recv             0.000000 | copy           0.012409 | convert time 0.000000 | train  0.025202
        L1  feature nbytes    1.51 GB | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes      58.28 MB | remote nbytes    1.06 GB
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S135]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.012409
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S135]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.000965 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.011376 | cache combine cache 0.000939 | cache combine remote 0.009432
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S135]
        p50.00_tail_logl2featcopy=0.012435
        p90.00_tail_logl2featcopy=0.012819
        p95.00_tail_logl2featcopy=0.012977
        p99.00_tail_logl2featcopy=0.029659
        p99.90_tail_logl2featcopy=0.030068
[CUDA] cuda: usage: 19.05 GB
Rank=2, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.37114381790161133
!!!!Train_data_list(with 17 items) enumerate latency: 4.76837158203125e-06, transfer latency: 0.26921701431274414
presamping
presamping takes 1.599370002746582
Rank=5, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.3753483295440674
!!!!Train_data_list(with 17 items) enumerate latency: 5.9604644775390625e-06, transfer latency: 0.2754943370819092
presamping
presamping takes 1.517932653427124
Rank=7, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.3770608901977539
!!!!Train_data_list(with 17 items) enumerate latency: 4.5299530029296875e-06, transfer latency: 0.28577709197998047
presamping
presamping takes 1.559645414352417
config:eval_tsp="2023-04-15 07:38:26"
config:num_worker=8
config:num_intra_size=4
config:root_dir=/nvme/songxiaoniu/graph-learning/wholegraph
config:graph_name=mag240m-homo
config:epochs=4
config:batchsize=8000
config:skip_epoch=2
config:local_step=125
config:presc_epoch=2
config:neighbors=10,25
config:hiddensize=256
config:num_layer=2
config:model=sage
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:lr=0.003
config:use_nccl=False
config:use_amp=True
config:use_collcache=True
config:cache_percentage=0.02
config:cache_policy=coll_cache_asymm_link
config:omp_thread_num=48
config:unsupervised=False
config:classnum=153
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7f018824e3d0>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=4, world_size=8
Rank=0, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.37363266944885254
!!!!Train_data_list(with 17 items) enumerate latency: 5.245208740234375e-06, transfer latency: 0.27327585220336914
epoch=4 total_steps=68
presamping
presamping takes 1.68965744972229
start training...
[Epoch 0], time=2.3312509059906006, loss=4.986895561218262
[Epoch 1], time=0.7107236385345459, loss=4.943203926086426
[Epoch 2], time=0.7110774517059326, loss=4.90230655670166
[Epoch 3], time=0.7087924480438232, loss=4.8641133308410645
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   95199 KB |  778691 KB |  152761 MB |  152668 MB |
|       from large pool |   83574 KB |  767284 KB |  151272 MB |  151191 MB |
|       from small pool |   11625 KB |   13945 KB |    1488 MB |    1477 MB |
|---------------------------------------------------------------------------|
| Active memory         |   95199 KB |  778691 KB |  152761 MB |  152668 MB |
|       from large pool |   83574 KB |  767284 KB |  151272 MB |  151191 MB |
|       from small pool |   11625 KB |   13945 KB |    1488 MB |    1477 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1938 MB |    1938 MB |    1938 MB |       0 B  |
|       from large pool |    1918 MB |    1918 MB |    1918 MB |       0 B  |
|       from small pool |      20 MB |      20 MB |      20 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   66592 KB |  640778 KB |  101681 MB |  101616 MB |
|       from large pool |   61834 KB |  637820 KB |  100045 MB |   99985 MB |
|       from small pool |    4758 KB |    7348 KB |    1635 MB |    1630 MB |
|---------------------------------------------------------------------------|
| Allocations           |      59    |      76    |   11453    |   11394    |
|       from large pool |      15    |      27    |    4477    |    4462    |
|       from small pool |      44    |      51    |    6976    |    6932    |
|---------------------------------------------------------------------------|
| Active allocs         |      59    |      76    |   11453    |   11394    |
|       from large pool |      15    |      27    |    4477    |    4462    |
|       from small pool |      44    |      51    |    6976    |    6932    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      27    |      27    |      27    |       0    |
|       from large pool |      17    |      17    |      17    |       0    |
|       from small pool |      10    |      10    |      10    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      40    |    5163    |    5133    |
|       from large pool |       9    |      18    |    3081    |    3072    |
|       from small pool |      21    |      26    |    2082    |    2061    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 4.463336 seconds
[EPOCH_TIME] 1.115834 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 0.710221 seconds
Rank=6, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.35973095893859863
!!!!Train_data_list(with 17 items) enumerate latency: 6.67572021484375e-06, transfer latency: 0.2688412666320801
presamping
presamping takes 1.615454912185669
Rank=3, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.37946224212646484
!!!!Train_data_list(with 17 items) enumerate latency: 6.4373016357421875e-06, transfer latency: 0.2795577049255371
presamping
presamping takes 1.455143690109253
Rank=1, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.3585069179534912
!!!!Train_data_list(with 17 items) enumerate latency: 5.9604644775390625e-06, transfer latency: 0.2699732780456543
presamping
presamping takes 1.4879589080810547
creating_intra_node_communicator root=4, local_size=4, world_size=8
Rank=4, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.36623716354370117
!!!!Train_data_list(with 17 items) enumerate latency: 6.198883056640625e-06, transfer latency: 0.2756040096282959
presamping
presamping takes 1.359178066253662

